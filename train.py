import random
import json 
import matplotlib.pyplot as plt 
import math 
import os
import torch 
import numpy as np 
from tqdm import tqdm 

from game import Game 
from agents.dqn_agent import DQNAgent 
from gridworld_generator import HotelGenerator 

# --- STATE REPRESENTATION FOR DQN ---
NUM_DQN_CHANNELS = 9 # MODIFIED: Owner, Player (Visible), Walls, Doors, Hallways, Room Floors, Hiding Spots, Exit, Sounds
SOUND_PERCEPTION_RADIUS = 7 # How far the owner can "hear" a sound event from its source tile (adjust as needed)

def get_dqn_state_representation(game_instance, grid_height, grid_width):
    """
    Creates a multi-channel tensor representation of the game state for the DQN.
    Output shape: (NUM_DQN_CHANNELS, grid_height, grid_width)
    """
    state_tensor_np = np.zeros((NUM_DQN_CHANNELS, grid_height, grid_width), dtype=np.float32)

    # Channel 0: Owner's Position
    if game_instance.owner_pos:
        orow, ocol = game_instance.owner_pos
        if 0 <= orow < grid_height and 0 <= ocol < grid_width:
            state_tensor_np[0, orow, ocol] = 1.0

    # Channel 1: Player's Position (Visible to Owner)
    owner_vision = game_instance.get_owner_vision_data() 
    if game_instance.player_pos:
        prow, pcol = game_instance.player_pos
        if (prow, pcol) in owner_vision and owner_vision[(prow, pcol)] == "PLAYER":
             if 0 <= prow < grid_height and 0 <= pcol < grid_width:
                state_tensor_np[1, prow, pcol] = 1.0
    
    # Iterate through the grid for static elements
    for r_idx in range(grid_height): # Renamed r to r_idx to avoid conflict
        for c_idx in range(grid_width): # Renamed c to c_idx
            tile = game_instance.grid[r_idx][c_idx]
            # Channel 2: Walls
            if tile == HotelGenerator.WALL:
                state_tensor_np[2, r_idx, c_idx] = 1.0
            # Channel 3: Doors
            elif tile == HotelGenerator.DOOR:
                state_tensor_np[3, r_idx, c_idx] = 1.0
            # Channel 4: Hallways
            elif tile == HotelGenerator.HALLWAY:
                state_tensor_np[4, r_idx, c_idx] = 1.0
            # Channel 5: Room Floors
            elif tile == HotelGenerator.ROOM_FLOOR:
                state_tensor_np[5, r_idx, c_idx] = 1.0
            # Channel 6: Hiding Spots
            elif tile == HotelGenerator.HIDING_SPOT:
                state_tensor_np[6, r_idx, c_idx] = 1.0
            # Channel 7: Exit Position
            if game_instance.exit_pos and r_idx == game_instance.exit_pos[0] and c_idx == game_instance.exit_pos[1]:
                 state_tensor_np[7, r_idx, c_idx] = 1.0

    # Channel 8: Sounds (from game_instance.sound_alerts)
    # These sounds are typically those generated by the player in their last set of moves,
    # or by the owner if this function is called after the owner's move (for s').
    for sound_event in game_instance.sound_alerts:
        # We are interested in any door sounds for the owner's perception
        if 'DOOR' in sound_event['type'].upper(): # Catches 'DOOR_OPENING', 'DOOR_CLOSING', 'DOOR_USAGE_PLAYER'
            sr, sc = sound_event['pos']
            # Mark the sound source and a radius around it
            for r_offset in range(-SOUND_PERCEPTION_RADIUS, SOUND_PERCEPTION_RADIUS + 1):
                for c_offset in range(-SOUND_PERCEPTION_RADIUS, SOUND_PERCEPTION_RADIUS + 1):
                    if r_offset**2 + c_offset**2 <= SOUND_PERCEPTION_RADIUS**2:
                        sound_r, sound_c = sr + r_offset, sc + c_offset
                        if 0 <= sound_r < grid_height and 0 <= sound_c < grid_width:
                            state_tensor_np[8, sound_r, sound_c] = 1.0
                 
    return torch.from_numpy(state_tensor_np)

# --- END STATE REPRESENTATION ---


def train_agent():
    # --- DIRECTORY SETUP ---
    MODEL_DIR = "models"
    PLOTS_DIR = "plots"
    os.makedirs(MODEL_DIR, exist_ok=True)
    os.makedirs(PLOTS_DIR, exist_ok=True)
    # --- END DIRECTORY SETUP ---

    # Game and Agent parameters
    GRID_HEIGHT = 30
    GRID_WIDTH = 40
    PLAYER_MOVES_PER_TURN = 2

    game_params = {
        'straightness_hallways': 0.9,
        'hall_loops': 12,
        'max_hallway_perc': 0.15,
        'max_rooms': 20,
        'room_min_size': 3,
        'room_max_size': 4,
        'max_hiding_spots_per_room': 1,
    }

    # DQN Agent parameters
    NUM_EPISODES = 10000
    MAX_TURNS_PER_EPISODE = 400 # You had this at 400 for testing
    LEARNING_RATE = 0.0005
    DISCOUNT_FACTOR = 0.99
    EXPLORATION_RATE_INITIAL = 1.0
    EXPLORATION_DECAY_RATE = 0.9999
    MIN_EXPLORATION_RATE = 0.05
    REPLAY_BUFFER_SIZE = 50000
    BATCH_SIZE = 256
    TARGET_UPDATE_FREQUENCY = 1000
    LEARN_START_STEPS = 2500
    LEARN_EVERY_N_STEPS = 4

    # Rewards (can be tuned)
    REWARD_CATCH_PLAYER = 100
    REWARD_PLAYER_ESCAPES = -100
    REWARD_BUMP_WALL = -10
    REWARD_PER_STEP = -1
    REWARD_PROXIMITY_BASE = 5
    REWARD_PROXIMITY_STEALTH_BONUS = 10
    PROXIMITY_THRESHOLD = 5

    game_actions_list = [Game.MOVE_NORTH, Game.MOVE_SOUTH, Game.MOVE_EAST, Game.MOVE_WEST, Game.WAIT]

    owner_agent = DQNAgent(
        game_actions_list=game_actions_list,
        input_channels=NUM_DQN_CHANNELS,
        grid_height=GRID_HEIGHT,
        grid_width=GRID_WIDTH,
        learning_rate=LEARNING_RATE,
        discount_factor=DISCOUNT_FACTOR,
        exploration_rate_initial=EXPLORATION_RATE_INITIAL,
        exploration_decay_rate=EXPLORATION_DECAY_RATE,
        min_exploration_rate=MIN_EXPLORATION_RATE,
        replay_buffer_size=REPLAY_BUFFER_SIZE,
        batch_size=BATCH_SIZE,
        target_update_frequency=TARGET_UPDATE_FREQUENCY
    )

    model_load_path = os.path.join(MODEL_DIR, "dqn_owner_agent.pth")
    # owner_agent.load_model(model_load_path) # Comment out if starting fresh or if model is causing issues

    total_rewards_per_episode = []
    total_steps_taken = 0
    print("Starting DQN training...")

    with tqdm(range(NUM_EPISODES), unit="episode") as episode_pbar:
        for episode in episode_pbar:
            game = Game(width=GRID_WIDTH, height=GRID_HEIGHT, player_moves_per_turn=PLAYER_MOVES_PER_TURN, generator_params=game_params)
            game.print_grid_with_entities(player_pov=False)

            if game.exit_pos:
                player_knows_exit_pos = game.exit_pos
            else:
                player_knows_exit_pos = None
            
            current_owner_state_tensor = get_dqn_state_representation(game, GRID_HEIGHT, GRID_WIDTH)
            episode_reward = 0
            game_turn_count = 0 

            player_recent_hallway_pos = [] # For simple tabu search in hallways

            while not game.game_over and game_turn_count < MAX_TURNS_PER_EPISODE:
                current_entity = game.get_current_turn_entity()

                if current_entity == "PLAYER":
                    chosen_player_action = Game.WAIT 

                    player_r, player_c = game.player_pos
                    current_player_tile_type = game._get_tile(player_r, player_c)
                    
                    player_vision, _ = game.get_player_vision_data()
                    owner_seen_at = None
                    for loc, entity_type_in_vision in player_vision.items():
                        if entity_type_in_vision == "OWNER":
                            owner_seen_at = loc
                            break
                    
                    exit_seen_at_current_turn = None
                    if game.exit_pos and game.exit_pos in player_vision and player_vision[game.exit_pos] == HotelGenerator.EXIT:
                        exit_seen_at_current_turn = game.exit_pos
                        player_knows_exit_pos = game.exit_pos # Update knowledge if seen

                    # print(f"player_knows_exit_pos: {player_knows_exit_pos}, game.exit_pos: {game.exit_pos}")

                    # --- FLEEING LOGIC ---
                    if owner_seen_at:
                        
                        if episode < 5 and game.player_moves_taken_this_turn == 0: 
                            print(f"Ep {episode}: Player at ({player_r},{player_c}) FLEEING owner at {owner_seen_at}")
                        
                        flee_actions_with_dist = []
                        # Example simplified flee:
                        for action_key, (dr, dc) in Game.ACTION_DELTAS.items():
                            next_r, next_c = player_r + dr, player_c + dc
                            if game._is_walkable(next_r, next_c, "PLAYER"):
                                dist_to_owner = math.sqrt((next_r - owner_seen_at[0])**2 + (next_c - owner_seen_at[1])**2)
                                flee_actions_with_dist.append({'action': action_key, 'dist': dist_to_owner})
                        if flee_actions_with_dist:
                            flee_actions_with_dist.sort(key=lambda x: x['dist'], reverse=True) # Move away
                            if flee_actions_with_dist: chosen_player_action = flee_actions_with_dist[0]['action']

                    # --- EXIT LOGIC ---
                    elif player_knows_exit_pos is not None and game.exit_pos is not None:
                        global_target_exit_pos = player_knows_exit_pos
                        # STEP 1: PLAYER IN ROOM
                        current_room_obj = game.get_room_player_is_in(player_r, player_c)
                        if current_room_obj and current_player_tile_type == HotelGenerator.ROOM_FLOOR:
                            if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Player at ({player_r},{player_c}) in Room, seeking door to exit towards {global_target_exit_pos}")
                            
                            room_doors = game.get_doors_for_room(current_room_obj)
                            best_door_to_use = None
                            min_hallway_dist_to_exit = float('inf')

                            for door_info in room_doors:
                                hallway_pos = door_info['connected_hallway']
                                dist = math.sqrt((hallway_pos[0] - global_target_exit_pos[0])**2 + 
                                                    (hallway_pos[1] - global_target_exit_pos[1])**2)
                                if dist < min_hallway_dist_to_exit:
                                    min_hallway_dist_to_exit = dist
                                    best_door_to_use = door_info
                            
                            if best_door_to_use:
                                target_door_pos_on_grid = best_door_to_use['door_pos']
                                if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Best door is {target_door_pos_on_grid}, connected_hallway {best_door_to_use['connected_hallway']}")
                                
                                actions_to_door = []
                                for action_key, (dr, dc) in Game.ACTION_DELTAS.items():
                                    next_r, next_c = player_r + dr, player_c + dc
                                    if game.is_pos_in_room_or_door(next_r, next_c, current_room_obj, target_door_pos_on_grid) and \
                                        game._is_walkable(next_r, next_c, "PLAYER"):
                                        dist_to_door = math.sqrt((next_r - target_door_pos_on_grid[0])**2 + (next_c - target_door_pos_on_grid[1])**2)
                                        actions_to_door.append({'action': action_key, 'dist': dist_to_door, 'next_pos':(next_r,next_c)})
                                
                                if actions_to_door:
                                    actions_to_door.sort(key=lambda x: x['dist'])
                                    min_d = actions_to_door[0]['dist']
                                    best_options = [opt for opt in actions_to_door if opt['dist'] == min_d]
                                    chosen_player_action = random.choice(best_options)['action']
                                    if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Moving to room door {target_door_pos_on_grid} via {chosen_player_action}. Options: {best_options}")
                                else:
                                    if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Stuck trying to reach room door {target_door_pos_on_grid}. Waiting.")
                            else:
                                if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Could not find a best door. Exploring in room.")
                                # Fallback as above
                                valid_room_moves = [] # Simplified, copy from above if needed
                                if Game.WAIT not in valid_room_moves: valid_room_moves.append(Game.WAIT)
                                if valid_room_moves: chosen_player_action = random.choice(valid_room_moves)


                        # STATE 2: PLAYER IS IN HALLWAY/ON DOOR, OR AT EXIT - PATHFIND TO GLOBAL EXIT
                        elif current_player_tile_type == HotelGenerator.HALLWAY or \
                             current_player_tile_type == HotelGenerator.DOOR or \
                             (player_r, player_c) == global_target_exit_pos:
                            
                            if (player_r, player_c) == global_target_exit_pos:
                                if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Player AT EXIT ({player_r},{player_c}). Waiting for game to end.")
                                chosen_player_action = Game.WAIT # Game should end
                            else:
                                if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Player at ({player_r},{player_c}) in Hallway/Door, seeking global exit {global_target_exit_pos}")
                                
                                # Add current pos to tabu list if it's a hallway
                                if current_player_tile_type == HotelGenerator.HALLWAY:
                                    if (player_r, player_c) not in player_recent_hallway_pos:
                                        player_recent_hallway_pos.append((player_r, player_c))
                                    if len(player_recent_hallway_pos) > 3: # Keep last 3 hallway pos
                                        player_recent_hallway_pos.pop(0)
                                
                                approach_actions_with_dist = []
                                for action_key, (dr, dc) in Game.ACTION_DELTAS.items():
                                    next_r, next_c = player_r + dr, player_c + dc
                                    if game._is_walkable(next_r, next_c, "PLAYER"):
                                        # Avoid recently visited hallway tiles if trying to move
                                        if action_key != Game.WAIT and game._get_tile(next_r, next_c) == HotelGenerator.HALLWAY and (next_r, next_c) in player_recent_hallway_pos:
                                            if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Skipping {action_key} to {next_r},{next_c} as it's recent hallway pos.")
                                            continue # Skip this move as it's recently visited in hallway

                                        dist_to_exit = math.sqrt((next_r - global_target_exit_pos[0])**2 + (next_c - global_target_exit_pos[1])**2)
                                        approach_actions_with_dist.append({'action': action_key, 'dist': dist_to_exit, 'next_pos':(next_r,next_c)})
                                
                                if approach_actions_with_dist:
                                    approach_actions_with_dist.sort(key=lambda x: x['dist'])
                                    min_dist = approach_actions_with_dist[0]['dist']
                                    best_options = [opt for opt in approach_actions_with_dist if opt['dist'] == min_dist]

                                    # Anti-stuck: if only WAIT is best and not at exit, prefer non-WAIT slightly worse
                                    if len(best_options) == 1 and best_options[0]['action'] == Game.WAIT and (player_r, player_c) != global_target_exit_pos:
                                        non_wait_options = [opt for opt in approach_actions_with_dist if opt['action'] != Game.WAIT and opt['dist'] <= min_dist + 1.5] # Increased tolerance
                                        if non_wait_options:
                                            if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Player avoiding WAIT in hallway. Considering: {non_wait_options}")
                                            best_options = sorted(non_wait_options, key=lambda x: x['dist']) # Re-sort
                                            min_dist = best_options[0]['dist']
                                            best_options = [opt for opt in best_options if opt['dist'] == min_dist]


                                    if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Hallway best options to {global_target_exit_pos}: {best_options}")
                                    if best_options: chosen_player_action = random.choice(best_options)['action']
                                else:
                                     if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: No approach actions in hallway. Waiting.")
                        
                        else: # Knows exit, but not in room or hallway (e.g. on a hiding spot outside a room)
                            if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Player at ({player_r},{player_c}) on {current_player_tile_type}, knows exit {global_target_exit_pos}. Using general approach.")
                            # General approach (original logic)
                            actions_to_target = []
                            for action_key, (dr, dc) in Game.ACTION_DELTAS.items():
                                next_r, next_c = player_r + dr, player_c + dc
                                if game._is_walkable(next_r, next_c, "PLAYER"):
                                    dist = math.sqrt((next_r - global_target_exit_pos[0])**2 + (next_c - global_target_exit_pos[1])**2)
                                    actions_to_target.append({'action': action_key, 'dist': dist})
                            if actions_to_target:
                                actions_to_target.sort(key=lambda x: x['dist'])
                                min_d = actions_to_target[0]['dist']
                                best_options = [opt for opt in actions_to_target if opt['dist'] == min_d]
                                chosen_player_action = random.choice(best_options)['action']

                    else: # Random exploration (doesn't know exit, not fleeing)
                        if episode < 5 and game.player_moves_taken_this_turn == 0: print(f"Ep {episode}: Player at ({player_r},{player_c}) exploring randomly.")
                        valid_player_moves = []
                        for p_act_key, (dr_p, dc_p) in Game.ACTION_DELTAS.items():
                            next_r_p, next_c_p = player_r + dr_p, player_c + dc_p
                            if game._is_walkable(next_r_p, next_c_p, "PLAYER"):
                                valid_player_moves.append(p_act_key)
                        if not valid_player_moves and game._is_walkable(player_r, player_c, "PLAYER"):
                            valid_player_moves.append(Game.WAIT)
                        if valid_player_moves: chosen_player_action = random.choice(valid_player_moves)
                    
                    # Reset tabu list if player is no longer in a hallway or has completed its moves for the turn
                    if game._get_tile(player_r, player_c) != HotelGenerator.HALLWAY or \
                        game.player_moves_taken_this_turn + 1 >= game.player_moves_per_turn :
                        player_recent_hallway_pos = []


                    game.handle_player_turn(chosen_player_action)
                    current_owner_state_tensor = get_dqn_state_representation(game, GRID_HEIGHT, GRID_WIDTH) # Update state for owner

                elif current_entity == "OWNER":
                    valid_owner_actions = []
                    chosen_owner_action = owner_agent.choose_action(current_owner_state_tensor.unsqueeze(0), game, valid_owner_actions)
                    moved_successfully = game.handle_owner_turn(chosen_owner_action)
                    next_owner_state_tensor = get_dqn_state_representation(game, GRID_HEIGHT, GRID_WIDTH)
                    
                    current_reward = 0
                    if not moved_successfully:
                        current_reward = REWARD_BUMP_WALL
                    else:
                        current_reward = REWARD_PER_STEP

                    if game.player_pos and game.owner_pos:
                        distance_to_player = math.sqrt((game.owner_pos[0] - game.player_pos[0])**2 +
                                                       (game.owner_pos[1] - game.player_pos[1])**2)
                        if distance_to_player <= PROXIMITY_THRESHOLD:
                            proximity_reward_value = REWARD_PROXIMITY_BASE * (1 - (distance_to_player / PROXIMITY_THRESHOLD))
                            current_reward += proximity_reward_value
                            player_vision_data_for_stealth, _ = game.get_player_vision_data()
                            owner_seen_by_player = False
                            if game.owner_pos in player_vision_data_for_stealth and player_vision_data_for_stealth[game.owner_pos] == "OWNER":
                                owner_seen_by_player = True
                            if not owner_seen_by_player:
                                current_reward += REWARD_PROXIMITY_STEALTH_BONUS * (1 - (distance_to_player / PROXIMITY_THRESHOLD))

                    is_done_by_game_rules = game.game_over
                    is_done_by_timeout = (game_turn_count + 1) >= MAX_TURNS_PER_EPISODE
                    is_episode_terminated = is_done_by_game_rules or is_done_by_timeout
                    
                    if is_done_by_game_rules:
                        if game.winner == "OWNER":
                            current_reward = REWARD_CATCH_PLAYER
                        elif game.winner == "PLAYER":
                            current_reward = REWARD_PLAYER_ESCAPES
                    
                    owner_agent.store_transition(current_owner_state_tensor, chosen_owner_action, current_reward,
                                                 next_owner_state_tensor if not is_episode_terminated else None,
                                                 is_episode_terminated)
                    
                    current_owner_state_tensor = next_owner_state_tensor
                    episode_reward += current_reward
                    game_turn_count +=1
                    total_steps_taken += 1

                    if total_steps_taken > LEARN_START_STEPS and total_steps_taken % LEARN_EVERY_N_STEPS == 0:
                        owner_agent.learn()
            
            owner_agent.decay_exploration()
            total_rewards_per_episode.append(episode_reward)

            postfix_stats = {
                "Epsilon": f"{owner_agent.epsilon:.4f}",
                "Steps": total_steps_taken,
                "Buffer": len(owner_agent.replay_buffer),
                "Ep Reward": f"{episode_reward:.2f}"
            }
            if len(total_rewards_per_episode) >= 100:
                moving_avg_reward = np.mean(total_rewards_per_episode[-100:])
                postfix_stats["Avg Rew (100)"] = f"{moving_avg_reward:.2f}"
            elif total_rewards_per_episode:
                current_avg_reward = np.mean(total_rewards_per_episode)
                postfix_stats["Avg Rew (All)"] = f"{current_avg_reward:.2f}"
            episode_pbar.set_postfix(postfix_stats)

            if (episode + 1) % 100 == 0:
                model_save_path = os.path.join(MODEL_DIR, f"dqn_owner_agent_episode_{episode+1}.pth")
                owner_agent.save_model(model_save_path)
                plot_filename = os.path.join(PLOTS_DIR, f"dqn_rewards_plot_episode_{episode+1}.png")
                plt.figure(figsize=(10,5))
                plt.plot(total_rewards_per_episode)
                plt.title("DQN Owner's Rewards per Episode During Training")
                plt.xlabel("Episode")
                plt.ylabel("Total Reward for Owner")
                plt.grid(True)
                plt.savefig(plot_filename)
                plt.close()

    print("\n--- DQN TRAINING COMPLETE ---")
    final_model_save_path = os.path.join(MODEL_DIR, "dqn_owner_agent_final.pth")
    owner_agent.save_model(final_model_save_path)
    print(f"Final DQN model saved as {final_model_save_path}")

    final_plot_filename = os.path.join(PLOTS_DIR, "dqn_final_rewards_plot.png")
    plt.figure(figsize=(10,5))
    plt.plot(total_rewards_per_episode)
    plt.title("DQN Owner's Rewards per Episode (Full Training)")
    plt.xlabel("Episode")
    plt.ylabel("Total Reward for Owner")
    plt.grid(True)
    plt.savefig(final_plot_filename)
    print(f"Final DQN rewards plot saved as {final_plot_filename}")
    # plt.show() # Comment out if running non-interactively

def test_trained_agent(model_filename="dqn_owner_agent_final.pth"):
# ... (test_trained_agent function remains the same) ...
    MODEL_DIR = "models"
    model_filepath = os.path.join(MODEL_DIR, model_filename)

    print(f"\n--- RUNNING WITH TRAINED DQN AGENT (FROM {model_filepath}) ---")
    
    GRID_HEIGHT = 25 
    GRID_WIDTH = 30
    PLAYER_MOVES_PER_TURN = 2

    game_params = { 'max_rooms': 8, 'room_min_size': 3, 'room_max_size': 5 } 
    MAX_TURNS_PER_EPISODE = 400 

    game_actions_list = [Game.MOVE_NORTH, Game.MOVE_SOUTH, Game.MOVE_EAST, Game.MOVE_WEST, Game.WAIT]
    
    trained_agent = DQNAgent(
        game_actions_list=game_actions_list,
        input_channels=NUM_DQN_CHANNELS,
        grid_height=GRID_HEIGHT,
        grid_width=GRID_WIDTH,
        exploration_rate_initial=0.0, 
        min_exploration_rate=0.0 
    )
    trained_agent.load_model(model_filepath)
    trained_agent.q_network.eval() 

    game = Game(width=GRID_WIDTH, height=GRID_HEIGHT, player_moves_per_turn=PLAYER_MOVES_PER_TURN, generator_params=game_params)
    game_turn_count = 0
    
    current_owner_state_tensor = get_dqn_state_representation(game, GRID_HEIGHT, GRID_WIDTH)

    while not game.game_over and game_turn_count < MAX_TURNS_PER_EPISODE:
        print(f"\n--- Game Turn {game_turn_count + 1} ---")
        current_entity = game.get_current_turn_entity()
        
        if current_entity == "PLAYER":
            game.print_grid_with_entities(player_pov=True)
            action_map = {'n': Game.MOVE_NORTH, 's': Game.MOVE_SOUTH, 'e': Game.MOVE_EAST, 'w': Game.MOVE_WEST, 'x': Game.WAIT}
            usr_input = input("Player action (n,s,e,w,x for wait): ").lower()
            action = action_map.get(usr_input)
            
            if action: 
                game.handle_player_turn(action)
            else: 
                print("Invalid input. Player waits.")
                game.handle_player_turn(Game.WAIT)
            current_owner_state_tensor = get_dqn_state_representation(game, GRID_HEIGHT, GRID_WIDTH) 
        
        elif current_entity == "OWNER":
            print("Owner (trained DQN) is thinking...")
            valid_owner_actions = [] 
            for owner_act_key in game_actions_list:
                if owner_act_key == Game.WAIT:
                    valid_owner_actions.append(owner_act_key)
                    continue
                dr_o, dc_o = Game.ACTION_DELTAS[owner_act_key]
                next_r_o, next_c_o = game.owner_pos[0] + dr_o, game.owner_pos[1] + dc_o
                if game._is_walkable(next_r_o, next_c_o, "OWNER"):
                    valid_owner_actions.append(owner_act_key)
            if not valid_owner_actions: valid_owner_actions.append(Game.WAIT)

            owner_action = trained_agent.choose_action(current_owner_state_tensor.unsqueeze(0), game, valid_owner_actions)
            print(f"Owner chose: {owner_action}")
            game.handle_owner_turn(owner_action)
            current_owner_state_tensor = get_dqn_state_representation(game, GRID_HEIGHT, GRID_WIDTH)
            game_turn_count += 1
        
        if current_entity == "OWNER" or game.get_current_turn_entity() == "OWNER":
             game.print_grid_with_entities(player_pov=False)

    print("\n--- FINAL TRAINED DQN AGENT GAME STATE ---")
    game.print_grid_with_entities(player_pov=False)
    if game.game_over: 
        print(f"Result: {game.winner} wins after {game_turn_count} owner turns!")
    elif game_turn_count >= MAX_TURNS_PER_EPISODE:
        print(f"Max turns ({MAX_TURNS_PER_EPISODE}) reached in test run. No winner.")


if __name__ == '__main__':
    train_agent()
    # print("\n\nStarting DQN test phase...")
    # test_trained_agent()